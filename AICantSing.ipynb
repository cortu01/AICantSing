{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.conda (Python 3.9.6)\n",
    "%pip install notebook ipykernel\n",
    "\n",
    "%pip install portaudio\n",
    "%pip install pyaudio\n",
    "%pip install openai pyaudio numpy\n",
    "%pip install python-dotenv\n",
    "#%pip install opencv\n",
    "#%pip install \"urllib3<2\"\n",
    "\n",
    "%pip install opencv-python\n",
    "\n",
    "%pip install keyboard\n",
    "%pip install pynput\n",
    "\n",
    "%pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing dependencies for local AI models\n",
    "%pip install transformers\n",
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is used to display the promo images at the bottom left and right of the screen.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths and captions\n",
    "PROMO1_PATH = 'promo1.png'\n",
    "PROMO2_PATH = 'promo2.png'\n",
    "PROMO1_CAPTION = \"AI & Storytelling\"\n",
    "PROMO2_CAPTION = \"Code\"\n",
    "\n",
    "def add_caption_below(image, caption, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.7, thickness=2, pad=10):\n",
    "    text_size, _ = cv2.getTextSize(caption, font, font_scale, thickness)\n",
    "    w, h = text_size\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    caption_img = np.ones((h + 2*pad, img_w, 3), dtype=np.uint8) * 255\n",
    "    x = (img_w - w) // 2\n",
    "    y = pad + h\n",
    "    cv2.putText(caption_img, caption, (x, y), font, font_scale, (0,0,0), thickness, cv2.LINE_AA)\n",
    "    return np.vstack([image, caption_img])\n",
    "\n",
    "# Load promo images\n",
    "promo1 = cv2.imread(PROMO1_PATH, cv2.IMREAD_COLOR)\n",
    "promo2 = cv2.imread(PROMO2_PATH, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Resize QR codes to same height (e.g., 180px)\n",
    "qr_height = 180\n",
    "def resize_keep_aspect(img, height):\n",
    "    h, w = img.shape[:2]\n",
    "    scale = height / h\n",
    "    return cv2.resize(img, (int(w*scale), height))\n",
    "promo1 = resize_keep_aspect(promo1, qr_height)\n",
    "promo2 = resize_keep_aspect(promo2, qr_height)\n",
    "\n",
    "# Add captions below each QR\n",
    "promo1 = add_caption_below(promo1, PROMO1_CAPTION)\n",
    "promo2 = add_caption_below(promo2, PROMO2_CAPTION)\n",
    "\n",
    "# Get screen size (using tkinter, works on Mac/Win/Linux)\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    screen_width = root.winfo_screenwidth()\n",
    "    screen_height = root.winfo_screenheight()\n",
    "except Exception:\n",
    "    screen_width, screen_height = 1920, 1080\n",
    "\n",
    "# Margin from the bottom and sides\n",
    "BOTTOM_MARGIN = 50  # pixels\n",
    "SIDE_MARGIN = 20    # pixels\n",
    "\n",
    "# Get image sizes\n",
    "h1, w1 = promo1.shape[:2]\n",
    "h2, w2 = promo2.shape[:2]\n",
    "\n",
    "# Calculate positions\n",
    "x1 = SIDE_MARGIN  # left window\n",
    "y1 = screen_height - h1 - BOTTOM_MARGIN\n",
    "\n",
    "x2 = screen_width - w2 - SIDE_MARGIN  # right window\n",
    "y2 = screen_height - h2 - BOTTOM_MARGIN\n",
    "\n",
    "# Show promo1 at bottom left\n",
    "cv2.imshow(\"Promo1\", promo1)\n",
    "cv2.moveWindow(\"Promo1\", x1, y1)\n",
    "\n",
    "# Show promo2 at bottom right\n",
    "cv2.imshow(\"Promo2\", promo2)\n",
    "cv2.moveWindow(\"Promo2\", x2, y2)\n",
    "\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the starter image\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import the function from utils.py\n",
    "from utils import load_starter_image\n",
    "\n",
    "# Load and display the starter image\n",
    "success = load_starter_image()\n",
    "\n",
    "if success:\n",
    "    print(\"Starter image loaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load starter image. Make sure 'starter.png' exists in the root folder.\")\n",
    "\n",
    "# Keep the window open\n",
    "import cv2\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force override cache location BEFORE importing aist\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set environment variables at the system level\n",
    "new_cache_dir = os.path.expanduser(\"~/Documents/huggingface_cache\")\n",
    "os.environ['HF_HOME'] = new_cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = new_cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = new_cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = new_cache_dir\n",
    "\n",
    "# Also set the cache directory for diffusers specifically\n",
    "os.environ['DIFFUSERS_CACHE'] = new_cache_dir\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(new_cache_dir, exist_ok=True)\n",
    "print(f\"✓ Cache location set to: {new_cache_dir}\")\n",
    "\n",
    "# Verify the environment variables are set\n",
    "print(\"Environment variables:\")\n",
    "for var in ['HF_HOME', 'TRANSFORMERS_CACHE', 'HF_HUB_CACHE', 'DIFFUSERS_CACHE']:\n",
    "    print(f\"  {var}: {os.environ.get(var, 'NOT SET')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing local image generation models from the AI & storytelling course (which wrap Huggingface models).\n",
    "#This is to provide a free alternative to OpenAI calls (generally faster if you have a GPU, reduced image \n",
    "# quality, but arguably more interesting).\n",
    "\n",
    "#@title Install required packages\n",
    "#@markdown Run this first so that we can configure the notebook to have our code available.\n",
    "%pip install https://github.com/pkage/ai-storytelling-backstage/archive/main.zip#subdirectory=code/\n",
    "\n",
    "from aist.common import is_gpu_available\n",
    "\n",
    "\n",
    "print(f'GPU is {\"\" if is_gpu_available() else \"NOT \"}available on this instance.')\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from aist import image\n",
    "#from google.colab import files\n",
    "from aist.common import render_output_text\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def get_concat_h_blank(im1, im2, color=(0, 0, 0)):\n",
    "    dst = Image.new('RGB', (im1.width + im2.width, max(im1.height, im2.height)), color)\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    return dst\n",
    "\n",
    "#import locale\n",
    "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "#locale.getpreferredencoding = lambda do_setlocale=False: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PyTorch for Apple Silicon GPU\n",
    "import torch\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✓ Apple Silicon GPU (MPS) is available!\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠ MPS not available, using CPU\")\n",
    "\n",
    "# Test GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"Current device: {device}\")\n",
    "\n",
    "# Set the device globally for the session\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test local image generation\n",
    "\n",
    "PROMPT = 'a dank cellar with multiple stalactites running away from gas materials' #@param {type: 'string'}\n",
    "\n",
    "SEED     = 42   #@param {type: 'integer'}\n",
    "ROUNDS   = 3   #@param {type: 'integer'}\n",
    "HEIGHT   = 512  #@param {type: 'integer'}\n",
    "WIDTH    = 512  #@param {type: 'integer'}\n",
    "\n",
    "# Try to force GPU usage\n",
    "try:\n",
    "    result = image.stable_diffusion(\n",
    "        PROMPT,\n",
    "        rounds=ROUNDS,\n",
    "        dims=(WIDTH,HEIGHT),\n",
    "        seed=SEED,\n",
    "        accelerate=True  # This should enable GPU acceleration\n",
    "    )\n",
    "    print(\"✓ Image generation completed with GPU acceleration!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ GPU acceleration failed: {e}\")\n",
    "    print(\"Falling back to CPU...\")\n",
    "    # Fallback to CPU\n",
    "    result = image.stable_diffusion(\n",
    "        PROMPT,\n",
    "        rounds=ROUNDS,\n",
    "        dims=(WIDTH,HEIGHT),\n",
    "        seed=SEED,\n",
    "        accelerate=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated image\n",
    "print(\"Displaying the generated image...\")\n",
    "display(result)\n",
    "\n",
    "# Also save it to see where it goes\n",
    "result.save(\"test_generated_image.png\")\n",
    "print(\"Image saved as 'test_generated_image.png' in current directory\")\n",
    "\n",
    "# Show image info\n",
    "print(f\"Image size: {result.size}\")\n",
    "print(f\"Image mode: {result.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Display the local generated image in the same window as other generated images\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Convert PIL image to OpenCV format\n",
    "opencv_image = cv2.cvtColor(np.array(result), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Resize to match your window dimensions\n",
    "opencv_image = cv2.resize(opencv_image, (1536, 1536))\n",
    "\n",
    "# Display in the same window as other generated images\n",
    "cv2.imshow(\"Generated Image\", opencv_image)\n",
    "cv2.moveWindow(\"Generated Image\", 50, 20) \n",
    "cv2.waitKey(1)\n",
    "\n",
    "print(\"Image displayed in 'Generated Image' window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the microphone. Use the appropriate index for the microphone,\n",
    "# by setting it as the value for the variable \"DEVICE_INDEX\" in the main \n",
    "# loop (last cell in notebook, or in the .py version).\n",
    "import pyaudio\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "for i in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will need to add your OpenAIAPI key in the .env file, located in your AICAntSing folder.\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "print(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the API key.\n",
    "import openai\n",
    "client = openai.OpenAI(api_key=API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", # model to use from Models Tab\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"this is a test request, write a short poem\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Deprecated] This code is used to display the promo images on the screen. They show up near the top left together.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths and captions\n",
    "PROMO1_PATH = 'promo1.png'\n",
    "PROMO2_PATH = 'promo2.png'\n",
    "PROMO1_CAPTION = \"AI & Storytelling\"\n",
    "PROMO2_CAPTION = \"Code\"\n",
    "\n",
    "def add_caption_below(image, caption, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.7, thickness=2, pad=10):\n",
    "    text_size, _ = cv2.getTextSize(caption, font, font_scale, thickness)\n",
    "    w, h = text_size\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    caption_img = np.ones((h + 2*pad, img_w, 3), dtype=np.uint8) * 255\n",
    "    x = (img_w - w) // 2\n",
    "    y = pad + h\n",
    "    cv2.putText(caption_img, caption, (x, y), font, font_scale, (0,0,0), thickness, cv2.LINE_AA)\n",
    "    return np.vstack([image, caption_img])\n",
    "\n",
    "# Load promo images\n",
    "promo1 = cv2.imread(PROMO1_PATH, cv2.IMREAD_COLOR)\n",
    "promo2 = cv2.imread(PROMO2_PATH, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Resize QR codes to same height (e.g., 180px)\n",
    "qr_height = 180\n",
    "def resize_keep_aspect(img, height):\n",
    "    h, w = img.shape[:2]\n",
    "    scale = height / h\n",
    "    return cv2.resize(img, (int(w*scale), height))\n",
    "promo1 = resize_keep_aspect(promo1, qr_height)\n",
    "promo2 = resize_keep_aspect(promo2, qr_height)\n",
    "\n",
    "# Add captions below each QR\n",
    "promo1 = add_caption_below(promo1, PROMO1_CAPTION)\n",
    "promo2 = add_caption_below(promo2, PROMO2_CAPTION)\n",
    "\n",
    "# Make them the same height\n",
    "h1, w1 = promo1.shape[:2]\n",
    "h2, w2 = promo2.shape[:2]\n",
    "if h1 != h2:\n",
    "    maxh = max(h1, h2)\n",
    "    promo1 = cv2.copyMakeBorder(promo1, 0, maxh-h1, 0, 0, cv2.BORDER_CONSTANT, value=[255,255,255])\n",
    "    promo2 = cv2.copyMakeBorder(promo2, 0, maxh-h2, 0, 0, cv2.BORDER_CONSTANT, value=[255,255,255])\n",
    "\n",
    "# Concatenate QR codes side by side\n",
    "promo_img = np.hstack([promo1, promo2])\n",
    "\n",
    "# Get screen size (using tkinter, works on Mac/Win/Linux)\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    screen_width = root.winfo_screenwidth()\n",
    "    screen_height = root.winfo_screenheight()\n",
    "except Exception:\n",
    "    screen_width, screen_height = 1920, 1080\n",
    "\n",
    "# Margin from the bottom (adjust as needed)\n",
    "BOTTOM_MARGIN = 30  # pixels\n",
    "\n",
    "# Calculate position for bottom left, with margin\n",
    "img_h, img_w = promo_img.shape[:2]\n",
    "x = 0  # left edge\n",
    "y = screen_height - img_h - BOTTOM_MARGIN\n",
    "if y < 0:\n",
    "    y = 0  # Don't go off screen\n",
    "\n",
    "# Show the promo image at bottom left, slightly up\n",
    "cv2.imshow(\"Promo\", promo_img)\n",
    "cv2.moveWindow(\"Promo\", x, y)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import time\n",
    "import keyboard\n",
    "import threading\n",
    "from pynput import keyboard as pynput_keyboard\n",
    "#import multiprocessing\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "USE_LOCAL_MODEL = False\n",
    "\n",
    "artist_song_info = input('Enter the artist and song or relevant details (leave blank to skip). then press Enter: ').strip()\n",
    "\n",
    "ABS_PATH = os.getenv(\"MY_ABS_PATH\")\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"ABS_PATH\")))\n",
    "from utils import generate_image  # Importing the function from a separate file\n",
    "\n",
    "API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "\n",
    "# Constants\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "DEVICE_INDEX = 2  # Change this to the desired input device index\n",
    "\n",
    "AUDIO_FILE = os.path.expanduser(\"~/Desktop/output.wav\")  # Temporary file for transcription. Save to Desktop\n",
    "#AUDIO_FILE = \"temp_audio.wav\"  # Temporary file for transcription\n",
    "\n",
    "# Global pause flag\n",
    "pause_flag = threading.Event()\n",
    "pause_flag.clear()\n",
    "\n",
    "def on_press(key):\n",
    "    try:\n",
    "        if key == pynput_keyboard.Key.f8:\n",
    "            pause_flag.set()\n",
    "            print(\"\\n--- PAUSED --- (Press F9 to resume)\")\n",
    "        elif key == pynput_keyboard.Key.f9:\n",
    "            if pause_flag.is_set():\n",
    "                pause_flag.clear()\n",
    "                print(\"Resumed.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Start listener in background\n",
    "listener = pynput_keyboard.Listener(on_press=on_press)\n",
    "listener.daemon = True\n",
    "listener.start()\n",
    "\n",
    "# Function to capture audio from the microphone\n",
    "def record_audio(duration=3):\n",
    "    \"\"\"Records audio for a specified duration and saves it to a file.\"\"\"\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK,\n",
    "                        input_device_index=DEVICE_INDEX)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * duration)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording stopped.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    with wave.open(AUDIO_FILE, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "# Function to transcribe recorded audio\n",
    "def transcribe_audio():\n",
    "    \"\"\"Uses OpenAI Whisper API to transcribe recorded audio.\"\"\"\n",
    "    client = openai.OpenAI(api_key=API_KEY)  # Add your API key here\n",
    "\n",
    "    with open(AUDIO_FILE, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "\n",
    "    return transcript.text  # Access the text field properly\n",
    "\n",
    "# Function to check for meaningful content and spawn a process\n",
    "def check_and_spawn_process(transcript, function_to_run):\n",
    "    import string\n",
    "    \n",
    "    # Remove spaces and punctuation, convert to lowercase\n",
    "    cleaned_text = transcript.lower().translate(str.maketrans('', '', string.punctuation + ' '))\n",
    "    \n",
    "    # Check if there are at least 4 meaningful characters\n",
    "    if len(cleaned_text) >= 4:\n",
    "        print(f\"Meaningful content detected! Generating image...\")\n",
    "        # Use the full transcript as the sentence\n",
    "        sentence = transcript.strip()\n",
    "        # Prepend artist/song info if provided\n",
    "        if artist_song_info:\n",
    "            prompt = f\"{artist_song_info}: {sentence}\"\n",
    "        else:\n",
    "            prompt = sentence\n",
    "        function_to_run(prompt, API_KEY, use_local_model=USE_LOCAL_MODEL)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Main function to handle live transcription\n",
    "needs_prompt = False\n",
    "\n",
    "def main():\n",
    "    global artist_song_info, needs_prompt\n",
    "    while True:\n",
    "        if pause_flag.is_set():\n",
    "            # Wait until unpaused\n",
    "            while pause_flag.is_set():\n",
    "                time.sleep(0.1)\n",
    "            # Set flag to prompt after current operation\n",
    "            needs_prompt = True\n",
    "\n",
    "        record_audio(duration=3)\n",
    "        transcript = transcribe_audio()\n",
    "        print(f\"Transcription: {transcript}\")\n",
    "        check_and_spawn_process(transcript, generate_image)\n",
    "        import cv2\n",
    "        cv2.waitKey(1)\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        # Prompt for new info if needed\n",
    "        if needs_prompt:\n",
    "            new_info = input(\"Enter new artist and song info (leave blank to keep current): \").strip()\n",
    "            if new_info:\n",
    "                artist_song_info = new_info\n",
    "            needs_prompt = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
